{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd1082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, AutoImageProcessor, ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395b4477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffaa6483449845a38780ea8d2c6f25f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/resnet-50'\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822a6c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7085aed30f2141449d49ccc103471572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------\n",
    "# functions\n",
    "# ----------------\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    return metric.compute(\n",
    "        predictions=np.argmax(eval_pred.predictions, axis=1), \n",
    "        references=eval_pred.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6a28cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Load Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b909483d9a46c1984073af638615a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16a9795048d411ba7c167690f147c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f7f970217e43c8a0672766e4b2e5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750feef1cdac48409259fa4741872cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953446a0f4cd4aa69972e690b20747d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a49a7ef2a4e45cc8d602f24f6c6f8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8262f27eac5446b591028c6c4f2423b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef42c67f77634e0faca96829f6b1b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec2415743c649f2a1d68f2e65ce806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a123f8c2b7744a0cbb5810cdb530103d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fe50fc2cd84f62ba9164ff9ce7e1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab22960806414cab10517901e2848a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f346a051f6654644989222d5fe37e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017e0e05b637473c9783a670046c3f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbeada27d26437c96a990072f92696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## prepare data\n",
    "print('-- Load Data...')\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"planet-imgs-original/split1/\")\n",
    "prepared_ds = dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7296b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare labels\n",
    "labels = [0,1]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9a60a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Load Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8820a4c02064ffab34e4fc2fb499b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08833b9009ad4976a93ce9be16df90cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/103M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load model\n",
    "print('-- Load Model...')\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ba31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training args\n",
    "s=0\n",
    "output_path = model_name.split(\"/\")[-1]+f'/split{s+1}'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    per_device_train_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",    \n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=10,\n",
    "    #fp16=True,\n",
    "    learning_rate=5e-6,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5bd57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training...\n"
     ]
    }
   ],
   "source": [
    "## trainer\n",
    "print('-- Training...')\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2517090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 124\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 620\n",
      "  Number of trainable parameters = 23512130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 66/620 01:14 < 10:45, 0.86 it/s, Epoch 1.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.692800</td>\n",
       "      <td>0.692773</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.701500</td>\n",
       "      <td>3.897073</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to resnet-50/split1/checkpoint-20\n",
      "Configuration saved in resnet-50/split1/checkpoint-20/config.json\n",
      "Model weights saved in resnet-50/split1/checkpoint-20/pytorch_model.bin\n",
      "Image processor saved in resnet-50/split1/checkpoint-20/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to resnet-50/split1/checkpoint-40\n",
      "Configuration saved in resnet-50/split1/checkpoint-40/config.json\n",
      "Model weights saved in resnet-50/split1/checkpoint-40/pytorch_model.bin\n",
      "Image processor saved in resnet-50/split1/checkpoint-40/preprocessor_config.json\n",
      "Deleting older checkpoint [resnet-50/split1/checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to resnet-50/split1/checkpoint-60\n",
      "Configuration saved in resnet-50/split1/checkpoint-60/config.json\n",
      "Model weights saved in resnet-50/split1/checkpoint-60/pytorch_model.bin\n",
      "Image processor saved in resnet-50/split1/checkpoint-60/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef23f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
