{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Follow the authentication workflow in \"satellite_imagery_exploration.ipynb\" in order to initialize ee\n",
    "\n",
    "Then load images if necessary using \"grab_images.py\"\n",
    "\n",
    "#### Images should already be loaded in image folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasrosenblatt/opt/miniconda3/envs/heuristic_fairness/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/lucasrosenblatt/opt/miniconda3/envs/heuristic_fairness/lib/python3.8/site-packages/torchvision/image.so, 6): Library not loaded: @rpath/libpng16.16.dylib\n",
      "  Referenced from: /Users/lucasrosenblatt/opt/miniconda3/envs/heuristic_fairness/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Reason: Incompatible library version: image.so requires version 56.0.0 or later, but libpng16.16.dylib provides version 54.0.0'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/heuristic_fairness/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Make sure you put data in folder and adjust \"buffelgrass_one_time\" path as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/ipykernel_60174/1217242569.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Abundance_Binary'] = df_filtered['Abundance_Name'].apply(lambda x: 1 if x == '75-94%' or x == '50-74%' or x == '95% or more' else 0)\n"
     ]
    }
   ],
   "source": [
    "buffelgrass_one_time = 'data/buffelgrass_one_time.csv'\n",
    "df = pd.read_csv(buffelgrass_one_time)\n",
    "df_filtered = df[['Observation_ID', 'Observation_Date', 'Create_Date','Latitude', 'Longitude', 'Abundance_Name']]\n",
    "df_filtered['Abundance_Binary'] = df_filtered['Abundance_Name'].apply(lambda x: 1 if x == '75-94%' or x == '50-74%' or x == '95% or more' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_path, image_path_ir):\n",
    "    # real color image\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert('RGB')\n",
    "        real_color = transforms.ToTensor()(img)\n",
    "\n",
    "    # infrared+ image\n",
    "    with Image.open(image_path_ir) as img:\n",
    "        img = img.convert('RGB')\n",
    "        infrared = transforms.ToTensor()(img)\n",
    "\n",
    "    # we stack the images vertically for passthrough\n",
    "    concatenated = torch.cat((real_color, infrared), dim=1)  # we could use dim=2 for horizontal stacking\n",
    "\n",
    "    # need to standardize the size and normalize for imagenet (because resnet trained on imagenet)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "    \n",
    "    return transform(concatenated)\n",
    "        \n",
    "class SatelliteImageDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.label_map = {category: i for i, category in enumerate(dataframe['Abundance_Binary'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = 'images/' + str(row['Observation_ID']) + '.png'\n",
    "        image_path_ir = 'images/' + str(row['Observation_ID']) + '_ir.png'\n",
    "        image = preprocess_images(image_path, image_path_ir)\n",
    "        label = float(row['Abundance_Binary'])\n",
    "        return image, torch.tensor(label, dtype=torch.float)\n",
    "    \n",
    "# train test split\n",
    "df_train, df_val = train_test_split(df_filtered, test_size=0.2, random_state=42, stratify=df_filtered['Abundance_Binary'])\n",
    "\n",
    "# dataset loaders for training and validation\n",
    "dataset = SatelliteImageDataset(df_train)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "val_dataset = SatelliteImageDataset(df_val)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False) \n",
    "\n",
    "# pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# finetune resnet\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 1)\n",
    "criterion = torch.nn.BCEWithLogitsLoss() #CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "num_epochs = 32\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0 \n",
    "\n",
    "# set to true if we want to keep the most accurate model vs. least lossy\n",
    "storing_val_accuracy = True \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        outputs = model(images)\n",
    "        # reshaping labels to match output shape\n",
    "        labels = labels.view(-1, 1) \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_dataloader:\n",
    "            outputs = model(images)\n",
    "            # reshaping labels to match output shape\n",
    "            labels = labels.view(-1, 1) \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # average loss for tracking\n",
    "    val_loss /= len(validation_dataloader)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    if storing_val_accuracy:\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with validation accuracy: {best_val_accuracy}%\")\n",
    "    else:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 1)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in validation_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the validation images: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6597d1ed23b894caf154b6750f098a8514a19e03807460ffd2d8425103778dc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
